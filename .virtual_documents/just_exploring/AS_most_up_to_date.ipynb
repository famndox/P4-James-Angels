{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1YoiQIEqJeLsSvNeZCpjQO58440RZt2YQ","timestamp":1724870899135}],"gpuType":"T4","authorship_tag":"ABX9TyMGT/JnTFWMgNGKUT+63ZZR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from transformers import (\n","    GPT2Tokenizer,\n","    TFGPT2LMHeadModel,\n","    DataCollatorForLanguageModeling,\n","    Trainer,\n","    TrainingArguments\n",")\n","import tensorflow as tf\n","from transformers import AutoTokenizer, TFBertForSequenceClassification\n","import pandas as pd\n"],"metadata":{"id":"ooy8i40WF-6-","executionInfo":{"status":"ok","timestamp":1724982081177,"user_tz":300,"elapsed":158,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Load BERT model and tokenizer for sentiment analysis\n","sentiment_model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n","sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n","sentiment_model = TFBertForSequenceClassification.from_pretrained(sentiment_model_name)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bBdwOiSGALg","executionInfo":{"status":"ok","timestamp":1724982085904,"user_tz":300,"elapsed":3271,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}},"outputId":"7a371196-c939-4c5e-a2c2-19445f7876fa"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n","- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"]}]},{"cell_type":"code","source":["# Load GPT-2 model and tokenizer for story generation\n","gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","gpt_model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WT4Dm5eyGAOG","executionInfo":{"status":"ok","timestamp":1724982089899,"user_tz":300,"elapsed":2790,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}},"outputId":"4c4f59b7-a211-461e-8bde-16b026876a29"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n","\n","All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"]}]},{"cell_type":"code","source":["# Set the pad token to the EOS token\n","gpt_tokenizer.pad_token = gpt_tokenizer.eos_token"],"metadata":{"id":"KoNPPdQTGAQX","executionInfo":{"status":"ok","timestamp":1724982092667,"user_tz":300,"elapsed":154,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Read stories from \"stories.csv\"\n","df = pd.read_csv(\"folk_tales_deduplicated.csv\", encoding='ISO-8859-1')\n","stories = df[\"text\"].tolist()\n"],"metadata":{"id":"YY89_l3cGL6N","executionInfo":{"status":"ok","timestamp":1724984007626,"user_tz":300,"elapsed":559,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["\n","# Preprocess the text\n","def preprocess_text(texts):\n","    return [text.lower().strip() for text in texts]\n","\n","processed_stories = preprocess_text(stories)"],"metadata":{"id":"FDaulUhgGAS1","executionInfo":{"status":"ok","timestamp":1724984012331,"user_tz":300,"elapsed":163,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# Function to create batches and pad them\n","def create_batches(texts, batch_size):\n","    for i in range(0, len(texts), batch_size):\n","        batch_texts = texts[i:i + batch_size]\n","        # Tokenize and pad each batch\n","        batch_inputs = sentiment_tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"tf\")\n","        yield batch_inputs"],"metadata":{"id":"eAiGgvjnGAU7","executionInfo":{"status":"ok","timestamp":1724984013569,"user_tz":300,"elapsed":218,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["# Process each batch separately for sentiment analysis\n","sentiments = []\n","for batch_inputs in create_batches(processed_stories, batch_size=10):\n","    # Generate predictions for the batch\n","    outputs = sentiment_model(**batch_inputs)\n","    batch_predictions = tf.argmax(outputs.logits, axis=-1)\n","    sentiments.extend(batch_predictions.numpy())"],"metadata":{"id":"lfooq4aRGAXo","executionInfo":{"status":"ok","timestamp":1724984147809,"user_tz":300,"elapsed":132327,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["\n","# Sentiment labels\n","sentiment_labels = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n","sentiment_counts = {label: 0 for label in sentiment_labels}"],"metadata":{"id":"eKjibKcCGAaE","executionInfo":{"status":"ok","timestamp":1724984278063,"user_tz":300,"elapsed":198,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["# Count occurrences of each sentiment\n","for sentiment in sentiments:\n","    sentiment_counts[sentiment_labels[sentiment]] += 1"],"metadata":{"id":"X2jQOGaeGAcf","executionInfo":{"status":"ok","timestamp":1724984279239,"user_tz":300,"elapsed":198,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# Show sentiment counts\n","print(\"Sentiment Counts:\")\n","for label, count in sentiment_counts.items():\n","    print(f\"{label}: {count}\")\n","\n","# Identify the highest count sentiment\n","highest_count_sentiment = max(sentiment_counts, key=sentiment_counts.get)\n","print(f\"Highest Count Sentiment: {highest_count_sentiment}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lKhDgfVZGAfJ","executionInfo":{"status":"ok","timestamp":1724984280750,"user_tz":300,"elapsed":245,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}},"outputId":"1a848996-0933-4432-d35b-a9085bb65398"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentiment Counts:\n","very negative: 588\n","negative: 399\n","neutral: 31\n","positive: 1002\n","very positive: 918\n","Highest Count Sentiment: positive\n"]}]},{"cell_type":"code","source":["def create_text_dataset(tokenizer, texts, block_size=512):\n","    tokenized_texts = [\n","        tokenizer(text, truncation=True, padding='max_length', max_length=block_size, return_tensors=\"tf\")[\"input_ids\"]\n","        for text in texts\n","    ]\n","\n","    # Convert all tensors to int32, which is the expected type for input IDs\n","    tokenized_texts = [tf.cast(tensor, tf.int32) for tensor in tokenized_texts]\n","\n","    # Filter out tensors with a length of 0\n","    filtered_texts = [tensor for tensor in tokenized_texts if tf.shape(tensor)[1] > 0]\n","\n","    if len(filtered_texts) < 2:\n","        raise ValueError(\"Not enough valid sequences to create a dataset. Please check your input data.\")\n","\n","    inputs = tf.concat(filtered_texts[:-1], axis=0)\n","    labels = tf.concat(filtered_texts[1:], axis=0)\n","\n","    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n","    return dataset\n","\n"],"metadata":{"id":"_PcGk8VlGAhn","executionInfo":{"status":"ok","timestamp":1724984287813,"user_tz":300,"elapsed":156,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["# Create and prepare the dataset for training\n","dataset = create_text_dataset(gpt_tokenizer, processed_stories)\n","dataset = dataset.batch(2)"],"metadata":{"id":"nFUrBbS4GAkG","executionInfo":{"status":"ok","timestamp":1724984909823,"user_tz":300,"elapsed":64584,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["# Define the optimizer, loss, and metrics for training\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)  # Reduced by a factor of 50. previously 5e-5(1e-6 is what it was changed to). This offers a slower convergence which can help capture more fine details.\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"],"metadata":{"id":"rQsLB5MJGAnC","executionInfo":{"status":"ok","timestamp":1724984911357,"user_tz":300,"elapsed":132,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["# Compile the model\n","gpt_model.compile(optimizer=optimizer, loss=loss_fn)"],"metadata":{"id":"nG_adK6KGmnL","executionInfo":{"status":"ok","timestamp":1724984913629,"user_tz":300,"elapsed":187,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["def create_text_dataset(tokenizer, texts, block_size=128):  # Reduce block size\n","    tokenized_texts = [\n","        tokenizer(text, truncation=True, padding='max_length', max_length=block_size, return_tensors=\"tf\")[\"input_ids\"]\n","        for text in texts\n","    ]\n","    ...\n","\n","\n","\n"],"metadata":{"id":"VPQ_um7TFicU","executionInfo":{"status":"ok","timestamp":1724984915187,"user_tz":300,"elapsed":186,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras import mixed_precision\n","\n","# Enable mixed precision\n","policy = mixed_precision.Policy('mixed_float16')\n","mixed_precision.set_global_policy(policy)\n","\n","# Compile the model with mixed precision\n","gpt_model.compile(optimizer=optimizer, loss=loss_fn)\n"],"metadata":{"id":"mvWb5e1RHvb4","executionInfo":{"status":"ok","timestamp":1724985099023,"user_tz":300,"elapsed":192,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["# Fine-tune the model\n","gpt_model.fit(dataset, epochs=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PaWo3EVGGmgn","outputId":"0b0a7990-4b62-4760-dcca-985a7894eda6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1022/1469 [===================>..........] - ETA: 7:02 - loss: 6.1298"]}]},{"cell_type":"code","source":["def generate_story(prompt, max_length=250):\n","    inputs = gpt_tokenizer(prompt, return_tensors=\"tf\", truncation=True, max_length=512)\n","    input_ids = inputs[\"input_ids\"]\n","    attention_mask = inputs[\"attention_mask\"]\n","\n","    outputs = gpt_model.generate(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        max_length=max_length,\n","        num_return_sequences=1,  # Generate multiple sequences\n","        pad_token_id=gpt_tokenizer.eos_token_id,\n","        no_repeat_ngram_size=2,\n","        num_beams=5,  # Use beam search with 5 beams\n","        temperature=1\n","    )\n","\n","    stories = [gpt_tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n","    return stories\n","\n"],"metadata":{"id":"quXOCCyqGmeg","executionInfo":{"status":"ok","timestamp":1724983076597,"user_tz":300,"elapsed":207,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# Create a prompt for story generation\n","input_prompt = f\"Once upon a time in a land filled with {highest_count_sentiment} emotions, there was a...\"\n"],"metadata":{"id":"0fnOV6_YGmbk","executionInfo":{"status":"ok","timestamp":1724983078343,"user_tz":300,"elapsed":234,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["# Generate and display a story\n","generated_story = generate_story(input_prompt, max_length=250)\n","print(\"Generated Story:\")\n","generated_story"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t65Ze0UdGAqB","executionInfo":{"status":"ok","timestamp":1724983155909,"user_tz":300,"elapsed":76961,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}},"outputId":"55d309bf-c5df-4cee-ae3b-58e031373902"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated Story:\n"]},{"output_type":"execute_result","data":{"text/plain":["['Once upon a time in a land filled with positive emotions, there was a... earth springs happiness springs everlasting bliss everlasting life everlasting happiness there happiness one happiness of happiness, bliss life happiness lived bliss lived of life lived happiness existed happinesst of existed bliss paradise life there paradise lived happy existed happyt last there life unhappy lived a life\\n day happiness land paradise paradise of unhappy land there unhappyt. untilted paradise.\\n to and happiness. to good.. good one happy land the happyted unhappy island happiness\\n until., happiness sea happinessland happinessless ofents happiness toents world happiness unhappy.entsents of theents heents\\nents the happiness and the world the unhappy,ents,,. happiness the the bliss of happy, the paradise happiness life the lonely of lonely. lonely, unhappy paradise\\n\\n happiness bliss land happiness happiness happy touched touched. touched world happy happiness touched the touched, touched his happiness realized paradise he paradise unhappy\\n lonely lonely paradise lonely happiness lonely happy. happy lonely new happiness he lonely unhappy lonely\\n. unhappy unhappy with unhappy happy happy unhappy new unhappy touched happy and touched unhappy crossed happy new happy\\n loneliness happiness loneliness happy loneliness unhappy loneliness\\n bliss new life for happiness new new paradise new he new lonely loneliness new loneliness lonely']"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# Save the GPT-2 model in the TensorFlow SavedModel format\n","gpt_model.save_pretrained('./gpt2_saved_model')\n","\n"],"metadata":{"id":"QVL1BIA2GvYN","executionInfo":{"status":"ok","timestamp":1724983862618,"user_tz":300,"elapsed":2686,"user":{"displayName":"Amber Soria","userId":"12367955806139307897"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cU80tYR5GvaN"},"execution_count":null,"outputs":[]}]}