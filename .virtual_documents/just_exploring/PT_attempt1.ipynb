import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
import numpy as np

# Load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertModel.from_pretrained('bert-base-uncased')









# Define custom dataset class
class FolkloreTalesDataset(tf.data.Dataset):
    def __init__(self, tales, labels):
        self.tales = tales
        self.labels = labels

    def __getitem__(self, idx):
        tale = self.tales[idx]
        label = self.labels[idx]
        inputs = tokenizer.encode_plus(
            tale,
            add_special_tokens=True,
            max_length=70,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='tf'
        )
        return inputs, label

    def __len__(self):
        return len(self.tales)

# Create dataset instances
train_dataset = FolkloreTalesDataset(train_tales, train_labels)
val_dataset = FolkloreTalesDataset(val_tales, val_labels)

# Compile model
model.compile(optimizer='adam', loss='masked_language_modeling')

# Fit model
model.fit(train_dataset, epochs=10, validation_data=val_dataset)
