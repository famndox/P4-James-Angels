{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fcf9539-2e11-42ae-9d8a-5521585d3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    TFGPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from transformers import AutoTokenizer, TFBertForSequenceClassification\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60133e24-80ad-4c40-b23e-611074192a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STOP'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"STOP\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c936058c-8cc2-4f8f-a72d-d3ec8dfca0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayoun\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model and tokenizer for sentiment analysis\n",
    "sentiment_model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
    "sentiment_model = TFBertForSequenceClassification.from_pretrained(sentiment_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7338e1ad-7445-431a-94f6-1f090e159566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer for story generation\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the pad token to the EOS token\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de2f40d7-e4f8-446e-82d8-be75cfa888b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2938\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>nation</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://fairytalez.com/momotaro/</td>\n",
       "      <td>japanese</td>\n",
       "      <td>Momotaro</td>\n",
       "      <td>If youll believe me there was a time when the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://fairytalez.com/the-birdcatcher/</td>\n",
       "      <td>serbian</td>\n",
       "      <td>The Birdcatcher</td>\n",
       "      <td>Near Constantinople there lived a man who knew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://fairytalez.com/sharing-joy-and-sorrow/</td>\n",
       "      <td>german</td>\n",
       "      <td>Sharing Joy and Sorrow</td>\n",
       "      <td>There was once a tailor who was a quarrelsome ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://fairytalez.com/the-punishment-of-gangana/</td>\n",
       "      <td>french</td>\n",
       "      <td>The Punishment of Gangana</td>\n",
       "      <td>Once upon a time there lived a king and queen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://fairytalez.com/the-peace-with-the-snakes/</td>\n",
       "      <td>north_american_native</td>\n",
       "      <td>The Peace with the Snakes</td>\n",
       "      <td>In those days there was a Piegan chief named O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source                 nation  \\\n",
       "0                   https://fairytalez.com/momotaro/               japanese   \n",
       "1            https://fairytalez.com/the-birdcatcher/                serbian   \n",
       "2     https://fairytalez.com/sharing-joy-and-sorrow/                 german   \n",
       "3  https://fairytalez.com/the-punishment-of-gangana/                 french   \n",
       "4  https://fairytalez.com/the-peace-with-the-snakes/  north_american_native   \n",
       "\n",
       "                       title  \\\n",
       "0                   Momotaro   \n",
       "1            The Birdcatcher   \n",
       "2     Sharing Joy and Sorrow   \n",
       "3  The Punishment of Gangana   \n",
       "4  The Peace with the Snakes   \n",
       "\n",
       "                                                text  \n",
       "0  If youll believe me there was a time when the ...  \n",
       "1  Near Constantinople there lived a man who knew...  \n",
       "2  There was once a tailor who was a quarrelsome ...  \n",
       "3  Once upon a time there lived a king and queen ...  \n",
       "4  In those days there was a Piegan chief named O...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and clean stories from \"stories.csv\"\n",
    "df = pd.read_csv(\"../Resources/Datasets/folk_tales_deduplicated.csv\", encoding='ISO-8859-1')\n",
    "column_values = df.iloc[:, 3]\n",
    "clean_values = column_values.apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s.]', '', str(x)))\n",
    "df.iloc[:, 3] = clean_values\n",
    "stories = df[\"text\"].tolist()\n",
    "print(len(stories))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e25c5f-0b8e-46e4-b8de-bc2b2c70eafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2938\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text\n",
    "def preprocess_text(texts):\n",
    "    return [text.lower().strip() for text in texts]\n",
    "\n",
    "processed_stories = preprocess_text(stories)\n",
    "print(len(processed_stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1e9b7c7-1e90-4c7c-8ed1-6174909738fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Counts:\n",
      "very negative: 667\n",
      "negative: 441\n",
      "neutral: 38\n",
      "positive: 954\n",
      "very positive: 838\n",
      "Highest Count Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# Function to create batches and pad them\n",
    "def create_batches(texts, batch_size):\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        # Tokenize and pad each batch\n",
    "        batch_inputs = sentiment_tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "        yield batch_inputs\n",
    "\n",
    "# Process each batch separately for sentiment analysis\n",
    "sentiments = []\n",
    "for batch_inputs in create_batches(processed_stories, batch_size=10):\n",
    "    # Generate predictions for the batch\n",
    "    outputs = sentiment_model(**batch_inputs)\n",
    "    batch_predictions = tf.argmax(outputs.logits, axis=-1)\n",
    "    sentiments.extend(batch_predictions.numpy())\n",
    "\n",
    "# Sentiment labels\n",
    "sentiment_labels = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
    "sentiment_counts = {label: 0 for label in sentiment_labels}\n",
    "\n",
    "# Count occurrences of each sentiment\n",
    "for sentiment in sentiments:\n",
    "    sentiment_counts[sentiment_labels[sentiment]] += 1\n",
    "\n",
    "# Show sentiment counts\n",
    "print(\"Sentiment Counts:\")\n",
    "for label, count in sentiment_counts.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "# Identify the highest count sentiment\n",
    "highest_count_sentiment = max(sentiment_counts, key=sentiment_counts.get)\n",
    "print(f\"Highest Count Sentiment: {highest_count_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "386c5aaf-b1fd-475d-8d01-14668b9556cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Def for dataset\n",
    "\n",
    "def create_text_dataset(tokenizer, texts, block_size=512):\n",
    "    tokenized_texts = [\n",
    "        tokenizer(text, truncation=True, padding='max_length', max_length=block_size, return_tensors=\"tf\")[\"input_ids\"]\n",
    "        for text in texts\n",
    "    ]\n",
    "\n",
    "    # Convert all tensors to int32, which is the expected type for input IDs\n",
    "    tokenized_texts = [tf.cast(tensor, tf.int32) for tensor in tokenized_texts]\n",
    "\n",
    "    # Filter out tensors with a length of 0\n",
    "    filtered_texts = [tensor for tensor in tokenized_texts if tf.shape(tensor)[1] > 0]\n",
    "\n",
    "    if len(filtered_texts) < 2:\n",
    "        raise ValueError(\"Not enough valid sequences to create a dataset. Please check your input data.\")\n",
    "\n",
    "    inputs = tf.concat(filtered_texts[:-1], axis=0)\n",
    "    labels = tf.concat(filtered_texts[1:], axis=0)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "    return dataset\n",
    "    \n",
    "# Create and prepare the dataset for training\n",
    "dataset = create_text_dataset(gpt_tokenizer, processed_stories)\n",
    "dataset = dataset.batch(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f4b4264-3306-4d9d-babc-f4ebebf1d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer, loss, and metrics for training\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)  # Reduced by a factor of 50. previously 5e-5(1e-6 is what it was changed to). This offers a slower convergence which can help capture more fine details.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Enable mixed precision\n",
    "policy = mixed_precision.Policy('float32')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Compile the model with mixed precision\n",
    "gpt_model.compile(optimizer='adam', loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c74b03b9-4257-444b-a4ab-7541960b0f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STOP'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"STOP\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3e04a8e-1710-43e6-89d0-2c893a61f10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x0000021272DFF060> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x0000021272DFF060> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1469/1469 [==============================] - 5933s 4s/step - loss: 6.2982\n",
      "Epoch 2/12\n",
      "1469/1469 [==============================] - 5488s 4s/step - loss: 6.2103\n",
      "Epoch 3/12\n",
      "1469/1469 [==============================] - 5480s 4s/step - loss: 6.1689\n",
      "Epoch 4/12\n",
      "1469/1469 [==============================] - 5438s 4s/step - loss: 6.0890\n",
      "Epoch 5/12\n",
      "1469/1469 [==============================] - 5468s 4s/step - loss: 5.9646\n",
      "Epoch 6/12\n",
      "1469/1469 [==============================] - 5489s 4s/step - loss: 5.8757\n",
      "Epoch 7/12\n",
      "1469/1469 [==============================] - 5525s 4s/step - loss: 5.8287\n",
      "Epoch 8/12\n",
      "1469/1469 [==============================] - 5529s 4s/step - loss: 5.7967\n",
      "Epoch 9/12\n",
      "1469/1469 [==============================] - 5625s 4s/step - loss: 5.7762\n",
      "Epoch 10/12\n",
      "1469/1469 [==============================] - 5621s 4s/step - loss: 5.7408\n",
      "Epoch 11/12\n",
      "1469/1469 [==============================] - 5641s 4s/step - loss: 5.7193\n",
      "Epoch 12/12\n",
      "1469/1469 [==============================] - 5774s 4s/step - loss: 5.6883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x2127a264990>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "gpt_model.fit(dataset, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5070508-e9fe-4a1e-b74c-c620f76664dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STOP'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"STOP\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afebc4c5-d91a-48c5-91b6-6e8cac5e1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story(prompt, max_length=250):\n",
    "    inputs = gpt_tokenizer(prompt, return_tensors=\"tf\", truncation=True, max_length=512)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    outputs = gpt_model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,  # Generate multiple sequences\n",
    "        pad_token_id=gpt_tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_beams=5,  # Use beam search with 5 beams\n",
    "        temperature=.7\n",
    "    )\n",
    "\n",
    "    stories = [gpt_tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57e31e06-8565-4ec5-8f9c-a11849c3de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Story:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Once upon a time in a land filled with positive emotions, there was a... the. the the and the a the to.. a and. and and a. to the he. he the of the was the it the his the in the said the that the they the had the i the you the for the on the as the one the him the with the so the not the at the if the came the is the but the me the this the when the will the do the have the out the she the there the be the would the them the man the went the all the my the very the her thethe the some the who the by the up the then the\\xa0 the are the little the were the into the down the from the good the no the what the o the go the which the time the we the day the an the about the over the can the get the away the']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a prompt for story generation\n",
    "input_prompt = f\"Once upon a time in a land filled with {highest_count_sentiment} emotions, there was a...\"\n",
    "\n",
    "# Generate and display a story\n",
    "generated_story = generate_story(input_prompt, max_length=250)\n",
    "print(\"Generated Story:\")\n",
    "generated_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c03a686-2628-45db-8390-eefafc763026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: The King heard a devil dwelled in the castle atop the hill well way as journey of had get. had from and the to the it give and a this him were then the his you was his the but son one night cameman made his. each the have as a had of an what the to one he me were to his you for.. so and. said he of have gold in to. eat a. my he do me right the who water door he soon prince a in were and said which soon found what to goat had eyes will your not began now down a over when they it look the you. never here he gold the. and never it put he day looked ain a a i the say said out tail replied an a a youngd and may him to to made as that fox dog it the been from them get the it to his will am to back it left when out a goar. will the a with. g orwell soon some go. you with and where said it heard and and and that who the off want and to you this people would began be who the would him if could i it and me replied his. and on tore by into he. did bread as. said away a you\n",
      "1: The King heard a devil dwelled in the castle atop the hill once many he went he it a. out go to all she a to of the h over wife young than. own were was would going a nor good not the for by great and of she of were so look to any the was sent thatman. the. is he heard of and man at and in them the they the. in why with not was till g they. this and can and them he the i thou a here gone go bread but children the at and on asked the of he the histhe the what came at at but i. fellow away had fromd man in a of set was very time the. it i the as he out he may was i the. and to where they and because himself woman where poor had a had and shall it you said big to i the a under as when i began. as to thought let he better the to with his very their just the where as dog and me thing farmer it him no not after take many hast again were he we so the the a great of saw way one two at said the over another kill said into this a to but was it when the come that had but. and she h until the into\n",
      "2: The King heard a devil dwelled in the castle atop the hill near at his on a and went it with to they to was. a.. toer neveredhe he the out and a and it a of daughter st to had into the as the is not the the bread.s what from boy he what to good in pleased know this the his as. come. is his woman and how than to knew in the with to. it look over woman whiching and to son the up to me was way he long. his saw and wolf there said too he. the bread i he. in where had together to the could t fellow to. than for answered him. before the have to jack o not gone going woman a where she have a no.. me. fire fellow the a go and his by always up of some said so the veryhe when here the his. ran say do. him thees he of to he very right you my m. is by the them. in little the no. boy will more here well in i or and this night been you to that and nothing and he the and have behind i on go for the left king should got said o the fox the them been the long into not as\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(12)\n",
    "\n",
    "# encode context the generation is conditioned on\n",
    "sample_input = 'The King heard a devil dwelled in the castle atop the hill'\n",
    "input_ids = gpt_tokenizer.encode(sample_input, return_tensors='tf')\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_outputs = gpt_model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=250, \n",
    "    top_k=369, \n",
    "    top_p=0.96, \n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, gpt_tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed8a9798-0b62-4e06-9a47-466e0cb53873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tell me a tale of the castle upon the hill... a had village three f. as in he tree side castmy the ofrow forestens put wereah dontak a high your against for and quite along came he sleepo something the human for some. we he needle i have and one i here he throughnowyou the he stone together the out of cast that mistress and mouse at my in as too friend beautiful them aah no and the tried father. off spotas boy o good sea that an his had the and the said egg the tomorrow but fast ind. i is quick be whatever. years to and a so in and women the and points and m indeed and good at gone the is tears you teach of their robbers the stopped a\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(2)\n",
    "\n",
    "# encode context the generation is conditioned on\n",
    "sample_input = 'Tell me a tale of the castle upon the hill...'\n",
    "input_ids = gpt_tokenizer.encode(sample_input, return_tensors='tf')\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = gpt_model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=150, \n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(gpt_tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4304b394-2d46-4882-8497-a0dd6a00325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "King father daughter child... there time as lived and near river good beautifulra uncle land walking man but he whenevern number another soon a that they small was by the as. here for mothers he go of had him far and went was. could end met under. her loved came thought had day with for angry re back distress crept. togetherbutplei the. spring was of two the appeared soon of mans st disan that were the said cat outside but him dam get gave not these them. she the. her mistress in would happen in your it the their may to into sang large came stranger gave was ah a that my day. hill came how. a feet his of with nothing says and master that sorrow bright one darkness ill the and lost\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "# encode context the generation is conditioned on\n",
    "sample_input = 'King father daughter child...'\n",
    "input_ids = gpt_tokenizer.encode(sample_input, return_tensors='tf')\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = gpt_model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=150, \n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(gpt_tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed0ea6c5-4b8e-4ba3-809a-92be62bd8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the GPT-2 model in the TensorFlow SavedModel format\n",
    "gpt_model.save_pretrained('./pt_bert_bot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b20abe-e147-4950-8dff-6fb0a7829494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
